{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Seungtae62/RL_Cart_Pole/blob/main/Section_9_deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gsN12V6QmGO4"
      },
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1>\n",
        "        Deep Q-Learning\n",
        "    </h1>\n",
        "</div>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "\n",
        "In this notebook, we extend the Q-Learning algorithm to use function approximators (Neural Networks). The resulting algorithm is known as Deep Q-Learning.\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
        "\n",
        "\n",
        "\n",
        "!pip install -qq gym==0.23.0\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import animation\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "\n",
        "class Maze(gym.Env):\n",
        "\n",
        "    def __init__(self, exploring_starts: bool = False,\n",
        "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.shaped_rewards = shaped_rewards\n",
        "        self.state = (size - 1, size - 1)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.maze = self._create_maze(size=size)\n",
        "        self.distances = self._compute_distances(self.goal, self.maze)\n",
        "        self.action_space = spaces.Discrete(n=4)\n",
        "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
        "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
        "\n",
        "        self.screen = None\n",
        "        self.agent_transform = None\n",
        "\n",
        "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
        "        reward = self.compute_reward(self.state, action)\n",
        "        self.state = self._get_next_state(self.state, action)\n",
        "        done = self.state == self.goal\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        if self.exploring_starts:\n",
        "            while self.state == self.goal:\n",
        "                self.state = tuple(self.observation_space.sample())\n",
        "        else:\n",
        "            self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
        "        assert mode in ['human', 'rgb_array']\n",
        "\n",
        "        screen_size = 600\n",
        "        scale = screen_size / 5\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))\n",
        "\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "\n",
        "                state = (row, col)\n",
        "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
        "                    if next_state not in self.maze[state]:\n",
        "\n",
        "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
        "                        # adjacent squares that are not connected).\n",
        "                        row_diff, col_diff = np.subtract(next_state, state)\n",
        "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
        "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
        "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
        "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
        "\n",
        "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
        "\n",
        "        # Add the geometry of the goal square to the viewer.\n",
        "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
        "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
        "\n",
        "        # Add the geometry of the agent to the viewer.\n",
        "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
        "        agent_col = int(scale * (self.state[1] + .5))\n",
        "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
        "\n",
        "        surf = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(surf, (0, 0))\n",
        "\n",
        "        return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "\n",
        "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        if self.shaped_rewards:\n",
        "            return - (self.distances[next_state] / self.distances.max())\n",
        "        return - float(state != self.goal)\n",
        "\n",
        "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
        "        reward = self.compute_reward(state, action)\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        done = next_state == self.goal\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
        "        if action == 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        elif action == 2:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 3:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Action value not supported:\", action)\n",
        "        if next_state in self.maze[state]:\n",
        "            return next_state\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
        "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
        "                for row in range(size) for col in range(size)}\n",
        "\n",
        "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
        "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
        "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
        "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
        "        walls = [\n",
        "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
        "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
        "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
        "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
        "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
        "        ]\n",
        "\n",
        "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
        "\n",
        "        for src, dst in obstacles:\n",
        "            maze[src].remove(dst)\n",
        "\n",
        "            if dst in maze:\n",
        "                maze[dst].remove(src)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_distances(goal: Tuple[int, int],\n",
        "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
        "        distances = np.full((5, 5), np.inf)\n",
        "        visited = set()\n",
        "        distances[goal] = 0.\n",
        "\n",
        "        while visited != set(maze):\n",
        "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
        "            closest = next(x for x in sorted_dst if x not in visited)\n",
        "            visited.add(closest)\n",
        "\n",
        "            for neighbour in maze[closest]:\n",
        "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "\n",
        "def test_agent(env, policy, episodes=10):\n",
        "    frames = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        while not done:\n",
        "            p = policy(state)\n",
        "            if isinstance(p, np.ndarray):\n",
        "                action = np.random.choice(4, p=p)\n",
        "            else:\n",
        "                action = p\n",
        "            next_state, reward, done, extra_info = env.step(action)\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            frames.append(img)\n",
        "            state = next_state\n",
        "\n",
        "    return display_video(frames)\n",
        "\n",
        "\n",
        "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def plot_stats(stats):\n",
        "    rows = len(stats)\n",
        "    cols = 1\n",
        "\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
        "\n",
        "    for i, key in enumerate(stats):\n",
        "        vals = stats[key]\n",
        "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
        "        if len(stats) > 1:\n",
        "            ax[i].plot(range(len(vals)), vals)\n",
        "            ax[i].set_title(key, size=18)\n",
        "        else:\n",
        "            ax.plot(range(len(vals)), vals)\n",
        "            ax.set_title(key, size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cost_to_go(env, q_network, xlabel=None, ylabel=None):\n",
        "    highx, highy = env.observation_space.high\n",
        "    lowx, lowy = env.observation_space.low\n",
        "    X = torch.linspace(lowx, highx, 100)\n",
        "    Y = torch.linspace(lowy, highy, 100)\n",
        "    X, Y = torch.meshgrid(X, Y)\n",
        "\n",
        "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
        "    Z = - q_network(q_net_input).max(dim=-1, keepdim=True)[0]\n",
        "    Z = Z.reshape(100, 100).detach().numpy()\n",
        "    X = X.numpy()\n",
        "    Y = Y.numpy()\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, cmap='jet', linewidth=0, antialiased=False)\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "    ax.set_xlabel(xlabel, size=14)\n",
        "    ax.set_ylabel(ylabel, size=14)\n",
        "    ax.set_title(\"Estimated cost-to-go\", size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_max_q(env, q_network, xlabel=None, ylabel=None, action_labels=[]):\n",
        "    highx, highy = env.observation_space.high\n",
        "    lowx, lowy = env.observation_space.low\n",
        "    X = torch.linspace(lowx, highx, 100)\n",
        "    Y = torch.linspace(lowy, highy, 100)\n",
        "    X, Y = torch.meshgrid(X, Y)\n",
        "    q_net_input = torch.stack([X.flatten(), Y.flatten()], dim=-1)\n",
        "    Z = q_network(q_net_input).argmax(dim=-1, keepdim=True)\n",
        "    Z = Z.reshape(100, 100).T.detach().numpy()\n",
        "    values = np.unique(Z.ravel())\n",
        "    values.sort()\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.xlabel(xlabel, size=14)\n",
        "    plt.ylabel(ylabel, size=14)\n",
        "    plt.title(\"Optimal action\", size=18)\n",
        "\n",
        "    im = plt.imshow(Z, cmap='jet')\n",
        "    colors = [im.cmap(im.norm(value)) for value in values]\n",
        "    patches = [mpatches.Patch(color=color, label=label) for color, label in zip(colors, action_labels)]\n",
        "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "    plt.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E4pxsu9-mPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXh1rh7YmGO7"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yIz69YQomGO7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn as nn\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CYkeXJCmGO8"
      },
      "source": [
        "## Create and prepare the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV0R_SojmGO8"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fCeFY5FamGO8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "501d192b-b89c-44f7-fb7f-03995c2547f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x784950465fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qi2Tk4qumGO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d73007e-c70f-463f-f43d-94b38a514583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State dimensions: 4, Number of actions: 2\n"
          ]
        }
      ],
      "source": [
        "state_dims = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "print(f\"State dimensions: {state_dims}, Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lRro8JfmGO9"
      },
      "source": [
        "### Prepare the environment to work with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tr4VrlvjmGO9"
      },
      "outputs": [],
      "source": [
        "class PreprocessEnv(gym.Wrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return torch.from_numpy(obs).unsqueeze(dim=0).float()\n",
        "\n",
        "    def step(self, action):\n",
        "        action = action.item()\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        next_state = torch.from_numpy(next_state).unsqueeze(dim=0).float()\n",
        "        reward = torch.tensor(reward).view(1, -1).float()\n",
        "        done = torch.tensor(done).view(1, -1)\n",
        "        return next_state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v87pAharmGO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821fffad-f176-49c6-d0a5-b150cbc275e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = PreprocessEnv(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "82nlQI26mGO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6231d53a-9250-4883-d7b2-3755fbd22a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample state: tensor([[-0.0109,  0.0312, -0.0337, -0.0015]])\n",
            "Next state: tensor([[-0.0103, -0.1634, -0.0337,  0.2804]]), Reward: tensor([[1.]]), Done: tensor([[False]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "state = env.reset()\n",
        "action = torch.tensor(0)\n",
        "next_state, reward, done, _ = env.step(action)\n",
        "print(f\"Sample state: {state}\")\n",
        "print(f\"Next state: {next_state}, Reward: {reward}, Done: {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aDkOllmmGO9"
      },
      "source": [
        "## Create the Q-Network and policy\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lPti0REmGO-"
      },
      "source": [
        "### Create the Q-Network: $\\hat q(s,a| \\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ABFi9TU9mGO-"
      },
      "outputs": [],
      "source": [
        "q_network = nn.Sequential(\n",
        "    nn.Linear(state_dims, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, num_actions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH4vUbtUmGO-"
      },
      "source": [
        "### Create the target Q-Network: $\\hat q(s, a|\\theta_{targ})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RGqbOfrEmGO-"
      },
      "outputs": [],
      "source": [
        "target_q_network = copy.deepcopy(q_network).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ1lw7UymGO-"
      },
      "source": [
        "### Create the exploratory policy: $b(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "T6E9kafymGO-"
      },
      "outputs": [],
      "source": [
        "def policy(state, epsilon=0.):\n",
        "    if torch.rand(1) < epsilon:\n",
        "        return torch.randint(num_actions, (1, 1))\n",
        "    else:\n",
        "        av = q_network(state).detach()\n",
        "        return torch.argmax(av, dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVHtSSQymGO-"
      },
      "source": [
        "## Create the Experience Replay buffer\n",
        "\n",
        "<br>\n",
        "<div style=\"text-align:center\">\n",
        "    <p>A simple buffer that stores transitions of arbitrary values, adapted from\n",
        "    <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#training\">this source.</a></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FjpSAtp4mGO-"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity=100000):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def insert(self, transition):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = transition\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        assert self.can_sample(batch_size)\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        batch = zip(*batch)\n",
        "        return [torch.cat(items) for items in batch]\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        return len(self.memory) >= batch_size * 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": false,
        "id": "pfIPpqjJmGO-"
      },
      "source": [
        "## Implement the algorithm\n",
        "\n",
        "</br></br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1VxwD6svmGO_"
      },
      "outputs": [],
      "source": [
        "def deep_q_learning(q_network, policy, episodes, alpha=0.0001, batch_size=32, gamma=0.99, epsilon=0.2):\n",
        "\n",
        "  optim = AdamW(q_network.parameters(), lr=alpha)\n",
        "  memory = ReplayMemory()\n",
        "  stats = {'MSE Loss': [], 'Returns': []}\n",
        "\n",
        "  for episode in tqdm(range(1, episodes + 1)):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    ep_return = 0\n",
        "    while not done:\n",
        "      action = policy(state, epsilon)\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "      memory.insert([state, action, reward, done, next_state])\n",
        "\n",
        "      if memory.can_sample(batch_size):\n",
        "        batch = memory.sample(batch_size)\n",
        "        state_b, action_b, reward_b, done_b, next_state_b = memory.sample(batch_size)\n",
        "        qsa_b = q_network(state_b).gather(1, action_b)\n",
        "\n",
        "        next_qsa_b = target_q_network(next_state_b)\n",
        "        next_qsa_b = torch.max(next_qsa_b, dim=-1, keepdim=True)[0]\n",
        "\n",
        "        target_b = reward_b + ~done_b * gamma * next_qsa_b\n",
        "        loss = F.mse_loss(qsa_b, target_b)\n",
        "\n",
        "        q_network.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        stats['MSE Loss'].append(loss)\n",
        "\n",
        "      state = next_state\n",
        "      ep_return += reward.item()\n",
        "\n",
        "    stats['Returns'].append(ep_return)\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "      target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "  return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "kbi8PEzlmGO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818ad4b5-f80f-4de4-cf92-3ab66e8196b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:11<00:00,  6.98it/s]\n"
          ]
        }
      ],
      "source": [
        "stats = deep_q_learning(q_network, policy, 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-3XR-_OmGO_"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCMwXERHmGO_"
      },
      "source": [
        "### Plot execution stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kWrIfjzkmGO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "e1cd8f0f-432a-46c0-d1c8-8b98e2d75fea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2e6f74fb3171>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpLObvs5mGO_"
      },
      "source": [
        "### Test the resulting agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LKyHEYyemGO_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "68c4d173-2599-4234-fbf3-ad07f2b7b76d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e6a01ce2437c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_agent' is not defined"
          ]
        }
      ],
      "source": [
        "test_agent(env, policy, episodes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK-lKC9RmGO_"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y5r_670mGO_"
      },
      "source": [
        "[[1] Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}